<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tommaso Colella | SWE</title>
    <link>https://gioleppe.github.io/</link>
    <description>Recent content on Tommaso Colella | SWE</description>
    <generator>Hugo -- 0.148.0</generator>
    <language>en</language>
    <lastBuildDate>Sun, 27 Apr 2025 23:20:50 +0000</lastBuildDate>
    <atom:link href="https://gioleppe.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Faster, please! (Or, how I reverse engineered an app to cater to my needs)</title>
      <link>https://gioleppe.github.io/posts/faster-please/</link>
      <pubDate>Sun, 27 Apr 2025 23:20:50 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/faster-please/</guid>
      <description>&lt;p&gt;I love listening to podcasts, and news in general. For a couple of years, I&amp;rsquo;ve been subscribed to a somewhat-niche (but really high quality) Italian weekly magazine. I like their Android application, and I use its Text To Speech functionality, but there&amp;rsquo;s a big problem with that: it&amp;rsquo;s impossible to increase the audio reproduction speed.&lt;/p&gt;
&lt;p&gt;I usually listen to articles while doing something else, such as washing the dishes, or commuting to the office. This means I only have a fixed amount of time for this activity. Because of these time restrictions, &lt;strong&gt;I like to cram as much content as possible in my daily listening&lt;/strong&gt;. I also get bored very quickly when someone speaks slowly (sorry, I&amp;rsquo;m engineered like this!). That&amp;rsquo;s exactly why &lt;strong&gt;I set the speed to 2x&lt;/strong&gt; for most of the podcasts I&amp;rsquo;m listening to. Being unable to do the same with the magazine&amp;rsquo;s articles really bothered me.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dirt-cheap Computer Science books</title>
      <link>https://gioleppe.github.io/posts/used-books/</link>
      <pubDate>Tue, 26 Mar 2024 13:11:50 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/used-books/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re a bookworm like me, you can&amp;rsquo;t resist the fascination of the paper medium. Reading a proper book is a much more relaxing experience than reading on a screen. Some &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6618184/&#34;&gt;studies&lt;/a&gt; also found a correlation between phone usage in bed and increased risk of having poor sleep quality. These are some reasons I&amp;rsquo;ve always preferred reading from paper books, especially at nighttime.&lt;/p&gt;
&lt;p&gt;Computer Science books are often expensive, and one could always object that paper production has an undeniable carbon footprint. You could go to a public library, which I personally do, and it is always a great idea. That said, some textbooks are perfect for frequent consultation: one would like to have them readily available.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Enforcing OPA policies with Terraform / OpenTofu in network-isolated Azure VNet - Part 1</title>
      <link>https://gioleppe.github.io/posts/opa-terraform-azure-one/</link>
      <pubDate>Wed, 06 Mar 2024 00:27:25 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/opa-terraform-azure-one/</guid>
      <description>&lt;p&gt;Recently, we had a couple of naming-related incidents at work. We respect certain naming conventions, but sometimes errors slip into our Terraform infrastructure&amp;rsquo;s code, and remediation may be costly.&lt;/p&gt;
&lt;p&gt;I had to find a way to simplify resource naming policy enforcement: at first, I thought about using Git Hooks (with a custom HCL parser) or local linters. Both of these approaches came with downsides that would make it unpractical to employ them on multiple repositories: first of all, the necessity to set up hooks or linter configurations in each repo.&lt;/p&gt;</description>
    </item>
    <item>
      <title>How to build a Summarizer Bot using GPT4, Azure Functions, GitHub Actions</title>
      <link>https://gioleppe.github.io/posts/summarizer-bot/</link>
      <pubDate>Tue, 27 Feb 2024 16:48:19 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/summarizer-bot/</guid>
      <description>&lt;p&gt;A friend asked me to create a Telegram channel to cross-post my articles onto (I did! You can find it &lt;a href=&#34;https://t.me/tomstechcorner&#34;&gt;here&lt;/a&gt;). The idea seemed nice, but I didn&amp;rsquo;t want to bother with writing a small summary for each article every time, and most of all, forgetful as I am, I would absolutely forget to post to the channel.&lt;/p&gt;
&lt;p&gt;I only had one choice: automate everything using GitHub Actions, Azure Function Apps, and Azure OpenAI, trying to have fun and learn something new in the process.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-hosted Copilot using Google Gemma and Ollama (&#43; Phi-2 comparison)</title>
      <link>https://gioleppe.github.io/posts/gemma-copilot/</link>
      <pubDate>Fri, 23 Feb 2024 21:07:36 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/gemma-copilot/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the &lt;a href=&#34;https://goo.gle/GemmaReport&#34;&gt;Gemma technical report&lt;/a&gt;, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ðŸ’¡According to the technical report accompanying Gemma&amp;rsquo;s announcement, the new models were trained using the same methodologies as Google&amp;rsquo;s top-tier Gemini models&lt;/p&gt;</description>
    </item>
    <item>
      <title>DIY self-hosted Copilot using Phi-2 and Ollama</title>
      <link>https://gioleppe.github.io/posts/diy-copilot-phi/</link>
      <pubDate>Mon, 19 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/diy-copilot-phi/</guid>
      <description>&lt;p&gt;On December 12th, &lt;a href=&#34;https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/&#34;&gt;Microsoft released their latest &amp;ldquo;SML&amp;rdquo; or &amp;ldquo;Small Language Model&amp;rdquo;&lt;/a&gt; Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat &amp;ldquo;green&amp;rdquo; model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it&amp;rsquo;s not &lt;strong&gt;THAT&lt;/strong&gt; small (it has 2.7 billion parameters), but it is at least 2 to 3 orders of magnitude smaller than the state-of-the-art model GPT4 by OpenAI (public data on the internals of GPT4 is not available, so we can only raise conjectures).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deploying AzureOpenAI Service using OpenTofu</title>
      <link>https://gioleppe.github.io/posts/azure-openai-service-opentofu/</link>
      <pubDate>Wed, 14 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/azure-openai-service-opentofu/</guid>
      <description>&lt;p&gt;Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we&amp;rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I&amp;rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi&amp;rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About Me</title>
      <link>https://gioleppe.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://gioleppe.github.io/about/</guid>
      <description>&lt;p&gt;Iâ€™m a Software Engineer based in Florence, Italy. I have a BSc in Computer Science from the University of Pisa, and I&amp;rsquo;m currently completing my MSc.&lt;/p&gt;
&lt;p&gt;Iâ€™m working as Tech Lead and Cloud Architect for Avanade.&lt;/p&gt;
&lt;p&gt;My interests lie in distributed architectures, LLMs, and programming languages. You can find me either writing code or listening to music by some unknown band.&lt;/p&gt;
&lt;h3 id=&#34;stuff-im-experienced-into&#34;&gt;Stuff I&amp;rsquo;m experienced into&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Azure and Cloud-native architectures&lt;/li&gt;
&lt;li&gt;DevOps methodologies (IaC, CI/CD)&lt;/li&gt;
&lt;li&gt;Developing and integrating REST APIs&lt;/li&gt;
&lt;li&gt;Integrating payment processors&lt;/li&gt;
&lt;li&gt;Working on legacy code&lt;/li&gt;
&lt;li&gt;Working with both SQL and noSQL DBMSs&lt;/li&gt;
&lt;li&gt;Docker and Docker Compose&lt;/li&gt;
&lt;li&gt;Unit Testing&lt;/li&gt;
&lt;li&gt;Developing Telegram bots (C# and Python bindings)&lt;/li&gt;
&lt;li&gt;git (git flow, conventional commits, hooks)&lt;/li&gt;
&lt;li&gt;Agile methodologies&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;languagesframeworks-ive-worked-with&#34;&gt;Languages/Frameworks I&amp;rsquo;ve worked with&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;C#/.NET&lt;/li&gt;
&lt;li&gt;OpenTofu/Terraform&lt;/li&gt;
&lt;li&gt;TS/Next.js&lt;/li&gt;
&lt;li&gt;Python/pandas&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;techs-i-would-like-to-work-with-in-the-future&#34;&gt;Techs I would like to work with in the future&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes&lt;/li&gt;
&lt;li&gt;Clojure&lt;/li&gt;
&lt;li&gt;F#&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
