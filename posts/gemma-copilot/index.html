<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Self-hosted Copilot using Google Gemma and Ollama (+ Phi-2 comparison) | Tommaso Colella | SWE</title>
<meta name=keywords content><meta name=description content="It&rsquo;s impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the Gemma technical report, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.
üí°According to the technical report accompanying Gemma&rsquo;s announcement, the new models were trained using the same methodologies as Google&rsquo;s top-tier Gemini models"><meta name=author content="Tommaso Colella"><link rel=canonical href=https://gioleppe.github.io/posts/gemma-copilot/><meta name=google-site-verification content="G-JHDYWZPGQF"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gioleppe.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gioleppe.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gioleppe.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gioleppe.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gioleppe.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gioleppe.github.io/posts/gemma-copilot/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/ld+json>{"@context":"https://schema.org/","@type":"Person","name":"Tommaso Colella","url":"https://gioleppe.github.io/","image":"https://media.licdn.com/dms/image/C5603AQEo-0VsShX7eg/profile-displayphoto-shrink_400_400/0/1612869300096?e=1676505600&v=beta&t=C0ZDSUriW6h0wK3Py6DoZ8viruvCLJdTg99jQCGsl9Y","sameAs":["https://www.linkedin.com/in/tommaso-colella-051012143/","https://github.com/gioleppe","https://www.facebook.com/amuchina"],"jobTitle":"Software Engineer","worksFor":{"@type":"Organization","name":"YOOX Net-a-Porter Group S.p.A"}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-JHDYWZPGQF"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JHDYWZPGQF",{anonymize_ip:!1})}</script><meta property="og:title" content="Self-hosted Copilot using Google Gemma and Ollama (+ Phi-2 comparison)"><meta property="og:description" content="It&rsquo;s impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the Gemma technical report, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.
üí°According to the technical report accompanying Gemma&rsquo;s announcement, the new models were trained using the same methodologies as Google&rsquo;s top-tier Gemini models"><meta property="og:type" content="article"><meta property="og:url" content="https://gioleppe.github.io/posts/gemma-copilot/"><meta property="og:image" content="https://gioleppe.github.io/images/emerald-cover-resized.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-23T21:07:36+00:00"><meta property="article:modified_time" content="2024-02-23T21:07:36+00:00"><meta property="og:site_name" content="Tommaso Colella"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gioleppe.github.io/images/emerald-cover-resized.jpg"><meta name=twitter:title content="Self-hosted Copilot using Google Gemma and Ollama (+ Phi-2 comparison)"><meta name=twitter:description content="It&rsquo;s impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the Gemma technical report, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.
üí°According to the technical report accompanying Gemma&rsquo;s announcement, the new models were trained using the same methodologies as Google&rsquo;s top-tier Gemini models"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gioleppe.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Self-hosted Copilot using Google Gemma and Ollama (+ Phi-2 comparison)","item":"https://gioleppe.github.io/posts/gemma-copilot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Self-hosted Copilot using Google Gemma and Ollama (+ Phi-2 comparison)","name":"Self-hosted Copilot using Google Gemma and Ollama (\u002b Phi-2 comparison)","description":"It\u0026rsquo;s impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the Gemma technical report, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.\nüí°According to the technical report accompanying Gemma\u0026rsquo;s announcement, the new models were trained using the same methodologies as Google\u0026rsquo;s top-tier Gemini models","keywords":[],"articleBody":"It‚Äôs impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the Gemma technical report, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.\nüí°According to the technical report accompanying Gemma‚Äôs announcement, the new models were trained using the same methodologies as Google‚Äôs top-tier Gemini models\nA few days back, I wrote an article on how to use Ollama to set up a Phi-2-based self-hosted copilot. It was natural for me to provide an update to my previous tutorial to help anyone interested in hands-on trying the new model.\nWe will use Gemma 2B since it is small enough to run on a typical consumer-level laptop.\nAfter setting up the copilot, we will use it to help us in Python coding and try to compare it with Phi-2.\nLet‚Äôs check some prerequisites, then jump straight to the code!\nPrerequisites We need to install the same tools as my previous Phi-2-based tutorial, so please install the following:\nVisual Studio Code Continue extension for Visual Studio Code WSL, if you are on Windows I will use concepts regarding copilots, Rest APIs, and programming in general in the Python coding part, but I will not explain those things in depth.\nThe copilot was tested on a Dell Latitude 7430 with 16GB of RAM and an Intel i5 1250p. Ollama ran in the WSL-2 environment, limited to 4GB of RAM. Performance may vary depending on your hardware.\nModel Setup Install Ollama and run the model The steps are the same as in the Phi-2 tutorial, so if you haven‚Äôt installed Ollama yet (or if you want to update Ollama like me), run:\n‚ûú ~ curl -fsSL https://ollama.com/install.sh | sh ‚ûú ~ ollama run gemma:2b I‚Äôm omitting the output of each command for brevity. The first command installs Ollama on your OS, and the second one automatically downloads the 2B instruction-tuned model and runs it interactively.\nIf everything works correctly, you should have an interactive prompt just like this one:\nYou can chat with the model to confirm it is working, if you want, otherwise continue to the next step.\nüí°Don‚Äôt close the interactive prompt, otherwise the next steps won‚Äôt work. If you prefer, you can run the model in the background, or set up a dedicated service.\nSetup Continue with Gemma Install Continue from the VS Code extension marketplace, then open the settings from the extension‚Äôs icon in VS Code to configure the Ollama provider.\nAssuming you are running Ollama on localhost, port 11434, modify the configuration file this way:\n{ \"model\": \"gemma:2b\", \"title\": \"Ollama-Gemma\", \"completionOptions\": {}, \"apiBase\": \"localhost:11434\", \"provider\": \"ollama\" } Now we can select the model from the dropdown in the left of the UI, and start testing the copilot.\nTesting the copilot We are going to test the copilot the same way we did with the Phi-based one. We will also use the same prompts.\nWe create an empty test.py file, then we use the ‚Äúctrl+shift+L‚Äù shortcut, to open the Quick Edit menu.\nWe provide the same prompt as we did with Phi-2: ‚Äúwrite a python app with a framework of your choice that accepts GET requests on a endpoint and answers back with a list of Users. The Users must be stored in a local data structure, avoid adding database related code‚Äù:\nHere is the generated code:\nimport flask import json app = flask. Flask(__name__) # Define a list of users users = [] @app.route(\"/\") def index(): return json.dumps(users) if __name__ == \"__main__\": app.run(debug=True) It is pretty close to what Phi-2 came up with. Here, Gemma preferred to use Python‚Äôs standard library in place of Flask‚Äôs JSON implementation to serialize the data.\nLet‚Äôs try the terminal debug feature with Gemma. If we run the application, we are going to get an error (we are clearly missing the Flask module):\nWe can use ‚Äúctrl+shift+R‚Äù to debug our terminal‚Äôs output with Continue and Gemma:\nGemma identified the problem and gave us a longer explanation than Phi‚Äôs. This might be better or worse depending on the task we are carrying out and personal preference. For now, it‚Äôs a close match between the two small-sized models.\nAfter installing the Flask module, and manually adding some data to the preceding code we can run again our test app, and confirm it is working as intended:\nPS C:\\Users\\tommaso.colella\\projects\\articles\\ollama-gemma\u003e curl http://127.0.0.1:5000 [{\"age\": 23, \"username\": \"billgates\"}, {\"age\": 29, \"username\": \"stevejobs\"}] Now we are going to ask Gemma a simple CRUD, so we highlight the code, press ‚Äúctrl+shift+L‚Äù and ask Continue the following prompt (which we already used with Phi-2) ‚Äúcan you please add endpoints to add users, delete users, and get a user by its index in the list?‚Äù\nGemma gave back the following code:\nimport flask import json app = flask.Flask(__name__) # Define a list of users users = [{\"age\": 23, \"username\": \"billgates\"}, {\"age\": 29, \"username\": \"stevejobs\"}] # Add a new user @app.route(\"/add_user\", methods=[\"POST\"]) def add_user(): data = flask.request.get_json() users.append(data) return json.dumps(users) # Delete a user by its index @app.route(\"/delete_user/\") def delete_user(index): users.pop(index) return json.dumps(users) # Get a user by its index @app.route(\"/get_user/\") def get_user(index): return json.dumps(users[index]) if __name__ == \"__main__\": app.run(debug=True) Gemma correctly interpreted my requirements, where Phi-2 made a couple of small errors: it didn‚Äôt include the __main__ entry point and got wrong the ‚Äúdelete-by-index‚Äù part. That said, Phi-2 included some limited error-related code in responses, which Gemma didn‚Äôt.\nOverall, I think Gemma performed slightly better in this task, but these tests are by no means conclusive and are limited by their own narrow scope.\nConclusions The Gemma model family, especially its smaller version Gemma 2B, is a very welcome addition to the world of LLMs (or, SLMs, as these models are often called). The openness of this model (and its cousin Phi-2) makes it very easy for researchers and developers to experiment with them.\nOverall I‚Äôm happy with the performance Gemma 2B demonstrated, in code completion,\nReferences Gemma: Google introduces new state-of-the-art open models (blog.google)\nGemma Technical Report\nPhi-2: The surprising power of small language models - Microsoft Research\nOllama\n","wordCount":"1031","inLanguage":"en","image":"https://gioleppe.github.io/images/emerald-cover-resized.jpg","datePublished":"2024-02-23T21:07:36Z","dateModified":"2024-02-23T21:07:36Z","author":{"@type":"Person","name":"Tommaso Colella"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gioleppe.github.io/posts/gemma-copilot/"},"publisher":{"@type":"Organization","name":"Tommaso Colella | SWE","logo":{"@type":"ImageObject","url":"https://gioleppe.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gioleppe.github.io/ accesskey=h title="Tommaso Colella | SWE (Alt + H)">Tommaso Colella | SWE</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://gioleppe.github.io/about title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gioleppe.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://gioleppe.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Self-hosted Copilot using Google Gemma and Ollama (+ Phi-2 comparison)</h1><div class=post-meta><span title='2024-02-23 21:07:36 +0000 UTC'>February 23, 2024</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;Tommaso Colella</div></header><figure class=entry-cover><img loading=eager src=https://gioleppe.github.io/images/emerald-cover-resized.jpg alt="Picture of an emerald"><p>Picture by <a href=https://unsplash.com/@girlwithredhat>Girl with red hat</a> on Unsplash</p></figure><div class=post-content><p>It&rsquo;s impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the <a href=https://goo.gle/GemmaReport>Gemma technical report</a>, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.</p><blockquote><p>üí°According to the technical report accompanying Gemma&rsquo;s announcement, the new models were trained using the same methodologies as Google&rsquo;s top-tier Gemini models</p></blockquote><p>A few days back, I wrote an article on how to use <a href=https://ollama.com/>Ollama</a> to set up a <a href=https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/>Phi-2-based</a> self-hosted copilot. It was natural for me to provide an update to my previous tutorial to help anyone interested in hands-on trying the new model.</p><p>We will use Gemma 2B since it is small enough to run on a typical consumer-level laptop.</p><p>After setting up the copilot, we will use it to help us in Python coding and try to compare it with Phi-2.</p><p>Let&rsquo;s check some prerequisites, then jump straight to the code!</p><hr><h4 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h4><p>We need to install the same tools as <a href=https://gioleppe.github.io/posts/diy-copilot-phi/>my previous Phi-2-based tutorial</a>, so please install the following:</p><ul><li><a href=https://code.visualstudio.com/>Visual Studio Code</a></li><li><a href=https://continue.dev/docs/quickstart>Continue</a> extension for Visual Studio Code</li><li><a href=https://learn.microsoft.com/en-us/windows/wsl/install>WSL</a>, if you are on Windows</li></ul><p>I will use concepts regarding copilots, Rest APIs, and programming in general in the Python coding part, but I will not explain those things in depth.</p><p>The copilot was tested on a Dell Latitude 7430 with 16GB of RAM and an Intel i5 1250p. Ollama ran in the WSL-2 environment, limited to 4GB of RAM. Performance may vary depending on your hardware.</p><hr><h4 id=model-setup>Model Setup<a hidden class=anchor aria-hidden=true href=#model-setup>#</a></h4><h5 id=install-ollama-and-run-the-model>Install Ollama and run the model<a hidden class=anchor aria-hidden=true href=#install-ollama-and-run-the-model>#</a></h5><p>The steps are the same as in the Phi-2 tutorial, so if you haven&rsquo;t installed Ollama yet (or if you want to update Ollama like me), run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>‚ûú  ~ curl -fsSL https://ollama.com/install.sh <span class=p>|</span> sh 
</span></span><span class=line><span class=cl>‚ûú  ~ ollama run gemma:2b
</span></span></code></pre></div><p>I&rsquo;m omitting the output of each command for brevity. The first command installs Ollama on your OS, and the second one automatically downloads the 2B instruction-tuned model and runs it interactively.</p><p>If everything works correctly, you should have an interactive prompt just like this one:</p><p><img loading=lazy src=images/interactive_prompt.png alt=interactive-prompt></p><p>You can chat with the model to confirm it is working, if you want, otherwise continue to the next step.</p><blockquote><p>üí°Don&rsquo;t close the interactive prompt, otherwise the next steps won&rsquo;t work. If you prefer, you can run the model in the background, or set up a dedicated service.</p></blockquote><h5 id=setup-continue-with-gemma>Setup Continue with Gemma<a hidden class=anchor aria-hidden=true href=#setup-continue-with-gemma>#</a></h5><p>Install Continue from the VS Code extension marketplace, then open the settings from the extension&rsquo;s icon in VS Code to configure the Ollama provider.</p><p>Assuming you are running Ollama on localhost, port 11434, modify the configuration file this way:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;gemma:2b&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;Ollama-Gemma&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;completionOptions&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;apiBase&#34;</span><span class=p>:</span> <span class=s2>&#34;localhost:11434&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;provider&#34;</span><span class=p>:</span> <span class=s2>&#34;ollama&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p>Now we can select the model from the dropdown in the left of the UI, and start testing the copilot.</p><h3 id=testing-the-copilot>Testing the copilot<a hidden class=anchor aria-hidden=true href=#testing-the-copilot>#</a></h3><p>We are going to test the copilot the same way we did with the Phi-based one. We will also use the same prompts.</p><p>We create an empty test.py file, then we use the &ldquo;ctrl+shift+L&rdquo; shortcut, to open the Quick Edit menu.</p><p>We provide the same prompt as we did with Phi-2: <em>&ldquo;write a python app with a framework of your choice that accepts GET requests on a endpoint and answers back with a list of Users. The Users must be stored in a local data structure, avoid adding database related code&rdquo;</em>:</p><p><img loading=lazy src=images/ollama_first_generation.gif alt=first-generation></p><p>Here is the generated code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>flask</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>flask</span><span class=o>.</span> <span class=n>Flask</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define a list of users</span>
</span></span><span class=line><span class=cl><span class=n>users</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s2>&#34;/&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>index</span><span class=p>():</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>users</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>app</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>debug</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>It is pretty close to what Phi-2 came up with. Here, Gemma preferred to use Python&rsquo;s standard library in place of Flask&rsquo;s JSON implementation to serialize the data.</p><p>Let&rsquo;s try the terminal debug feature with Gemma. If we run the application, we are going to get an error (we are clearly missing the Flask module):</p><p><img loading=lazy src=images/run_error.png alt=run-error></p><p>We can use &ldquo;ctrl+shift+R&rdquo; to debug our terminal&rsquo;s output with Continue and Gemma:</p><p><img loading=lazy src=images/debug_terminal.png alt=debug-terminal></p><p>Gemma identified the problem and gave us a longer explanation than Phi&rsquo;s. This might be better or worse depending on the task we are carrying out and personal preference. For now, it&rsquo;s a close match between the two small-sized models.</p><p>After installing the Flask module, and manually adding some data to the preceding code we can run again our test app, and confirm it is working as intended:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>PS </span><span class=n>C:</span><span class=p>\</span><span class=n>Users</span><span class=p>\</span><span class=n>tommaso</span><span class=p>.</span><span class=n>colella</span><span class=p>\</span><span class=n>projects</span><span class=p>\</span><span class=n>articles</span><span class=p>\</span><span class=nb>ollama-gemma</span><span class=p>&gt;</span> <span class=nb>curl </span><span class=n>http</span><span class=err>:</span><span class=p>//</span><span class=mf>127.0</span><span class=p>.</span><span class=py>0</span><span class=p>.</span><span class=mf>1</span><span class=err>:</span><span class=mf>5000</span>                                        
</span></span><span class=line><span class=cl><span class=p>[{</span><span class=s2>&#34;age&#34;</span><span class=err>:</span> <span class=mf>23</span><span class=p>,</span> <span class=s2>&#34;username&#34;</span><span class=err>:</span> <span class=s2>&#34;billgates&#34;</span><span class=p>},</span> <span class=p>{</span><span class=s2>&#34;age&#34;</span><span class=err>:</span> <span class=mf>29</span><span class=p>,</span> <span class=s2>&#34;username&#34;</span><span class=err>:</span> <span class=s2>&#34;stevejobs&#34;</span><span class=p>}]</span>
</span></span></code></pre></div><p>Now we are going to ask Gemma a simple CRUD, so we highlight the code, press &ldquo;ctrl+shift+L&rdquo; and ask Continue the following prompt (which we already used with Phi-2) <em>&ldquo;can you please add endpoints to add users, delete users, and get a user by its index in the list?&rdquo;</em></p><p><img loading=lazy src=images/ollama_add_crud.gif alt=first-generation></p><p>Gemma gave back the following code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>flask</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>json</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>flask</span><span class=o>.</span><span class=n>Flask</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Define a list of users</span>
</span></span><span class=line><span class=cl><span class=n>users</span> <span class=o>=</span> <span class=p>[{</span><span class=s2>&#34;age&#34;</span><span class=p>:</span> <span class=mi>23</span><span class=p>,</span> <span class=s2>&#34;username&#34;</span><span class=p>:</span> <span class=s2>&#34;billgates&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>         <span class=p>{</span><span class=s2>&#34;age&#34;</span><span class=p>:</span> <span class=mi>29</span><span class=p>,</span> <span class=s2>&#34;username&#34;</span><span class=p>:</span> <span class=s2>&#34;stevejobs&#34;</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Add a new user</span>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s2>&#34;/add_user&#34;</span><span class=p>,</span> <span class=n>methods</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;POST&#34;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_user</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>flask</span><span class=o>.</span><span class=n>request</span><span class=o>.</span><span class=n>get_json</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>users</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>users</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Delete a user by its index</span>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s2>&#34;/delete_user/&lt;int:index&gt;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>delete_user</span><span class=p>(</span><span class=n>index</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>users</span><span class=o>.</span><span class=n>pop</span><span class=p>(</span><span class=n>index</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>users</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Get a user by its index</span>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s2>&#34;/get_user/&lt;int:index&gt;&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_user</span><span class=p>(</span><span class=n>index</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>json</span><span class=o>.</span><span class=n>dumps</span><span class=p>(</span><span class=n>users</span><span class=p>[</span><span class=n>index</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s2>&#34;__main__&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>app</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>debug</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>Gemma correctly interpreted my requirements, where Phi-2 made a couple of small errors: it didn&rsquo;t include the <em>__main__</em> entry point and got wrong the &ldquo;delete-by-index&rdquo; part. That said, Phi-2 included some limited error-related code in responses, which Gemma didn&rsquo;t.</p><p>Overall, I think Gemma performed slightly better in this task, but these tests are by no means conclusive and are limited by their own narrow scope.</p><h3 id=conclusions>Conclusions<a hidden class=anchor aria-hidden=true href=#conclusions>#</a></h3><p>The Gemma model family, especially its smaller version Gemma 2B, is a very welcome addition to the world of LLMs (or, SLMs, as these models are often called). The openness of this model (and its cousin Phi-2) makes it very easy for researchers and developers to experiment with them.</p><p>Overall I&rsquo;m happy with the performance Gemma 2B demonstrated, in code completion,</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p><a href=https://blog.google/technology/developers/gemma-open-models/>Gemma: Google introduces new state-of-the-art open models (blog.google)</a></p><p><a href=https://goo.gle/GemmaReport>Gemma Technical Report</a></p><p><a href=https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/>Phi-2: The surprising power of small language models - Microsoft Research</a></p><p><a href=https://ollama.com/>Ollama</a></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://gioleppe.github.io/posts/diy-copilot-phi/><span class=title>Next ¬ª</span><br><span>DIY self-hosted Copilot using Phi-2 and Ollama</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://gioleppe.github.io/>Tommaso Colella | SWE</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>