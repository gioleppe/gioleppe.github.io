<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>DIY self-hosted Copilot using Phi-2 and Ollama | Tommaso Colella | SWE</title>
<meta name=keywords content><meta name=description content="On December 12th, Microsoft released their latest &ldquo;SML&rdquo; or &ldquo;Small Language Model&rdquo; Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat &ldquo;green&rdquo; model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it&rsquo;s not THAT small (it has 2."><meta name=author content="Tommaso Colella"><link rel=canonical href=https://gioleppe.github.io/posts/diy-copilot-phi/><meta name=google-site-verification content="G-JHDYWZPGQF"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gioleppe.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gioleppe.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gioleppe.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gioleppe.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gioleppe.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://gioleppe.github.io/posts/diy-copilot-phi/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/ld+json>{"@context":"https://schema.org/","@type":"Person","name":"Tommaso Colella","url":"https://gioleppe.github.io/","image":"https://media.licdn.com/dms/image/C5603AQEo-0VsShX7eg/profile-displayphoto-shrink_400_400/0/1612869300096?e=1676505600&v=beta&t=C0ZDSUriW6h0wK3Py6DoZ8viruvCLJdTg99jQCGsl9Y","sameAs":["https://www.linkedin.com/in/tommaso-colella-051012143/","https://github.com/gioleppe","https://www.facebook.com/amuchina"],"jobTitle":"Software Engineer","worksFor":{"@type":"Organization","name":"YOOX Net-a-Porter Group S.p.A"}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-JHDYWZPGQF"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JHDYWZPGQF",{anonymize_ip:!1})}</script><meta property="og:title" content="DIY self-hosted Copilot using Phi-2 and Ollama"><meta property="og:description" content="On December 12th, Microsoft released their latest &ldquo;SML&rdquo; or &ldquo;Small Language Model&rdquo; Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat &ldquo;green&rdquo; model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it&rsquo;s not THAT small (it has 2."><meta property="og:type" content="article"><meta property="og:url" content="https://gioleppe.github.io/posts/diy-copilot-phi/"><meta property="og:image" content="https://gioleppe.github.io/images/ollama_phi_cover_cropped.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-19T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-19T00:00:00+00:00"><meta property="og:site_name" content="Tommaso Colella"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gioleppe.github.io/images/ollama_phi_cover_cropped.jpg"><meta name=twitter:title content="DIY self-hosted Copilot using Phi-2 and Ollama"><meta name=twitter:description content="On December 12th, Microsoft released their latest &ldquo;SML&rdquo; or &ldquo;Small Language Model&rdquo; Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat &ldquo;green&rdquo; model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it&rsquo;s not THAT small (it has 2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gioleppe.github.io/posts/"},{"@type":"ListItem","position":2,"name":"DIY self-hosted Copilot using Phi-2 and Ollama","item":"https://gioleppe.github.io/posts/diy-copilot-phi/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"DIY self-hosted Copilot using Phi-2 and Ollama","name":"DIY self-hosted Copilot using Phi-2 and Ollama","description":"On December 12th, Microsoft released their latest \u0026ldquo;SML\u0026rdquo; or \u0026ldquo;Small Language Model\u0026rdquo; Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat \u0026ldquo;green\u0026rdquo; model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it\u0026rsquo;s not THAT small (it has 2.","keywords":[],"articleBody":"On December 12th, Microsoft released their latest ‚ÄúSML‚Äù or ‚ÄúSmall Language Model‚Äù Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat ‚Äúgreen‚Äù model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it‚Äôs not THAT small (it has 2.7 billion parameters), but it is at least 2 to 3 orders of magnitude smaller than the state-of-the-art model GPT4 by OpenAI (public data on the internals of GPT4 is not available, so we can only raise conjectures).\nüí°In the last months, interest in SMLs has been rapidly rising. What they really shine on, other than being suitable for deployment on edge devices such as mobile phones is performance per watt. Reducing the carbon footprint for these natural language models (for both the training and inference phases) is increasingly important in a world where companies and policymakers should be very considerate about climate change and actions to combat it.\nBeing an incurable nerd, I thought about doing some experiments with it, so I set up my plan to use it as a drop-in replacement for Github Copilot, at least to test its performance in a real-world-like scenario such as Python programming.\nI get it. Copilot is so cool and is based on a state-of-the-art model, so why would I replace it with a worse one? Well, there are a couple of good reasons one might use it in place of the bigger and stronger competition:\nAs a lightweight model, it can be self-hosted on a standard prosumer PC. No need to pay for loan-shark-priced internet usage next time you‚Äôre on an international flight and wanna kick up your ai-augmented code editor! You are 100% sure your data isn‚Äôt ending up in a future training dataset. Prompts won‚Äôt leave your device at all (or local network, as long as you properly self-host the model). It‚Äôs plainly fun to experiment with these things, so why not? As you will see, the process is straightforward. To quote (and slightly rephrase) Linus Torvalds, talk is cheap, let‚Äôs jump straight into the code!\nPrerequisites For our DIY copilot, we need a couple of tools first, so be sure to have the following installed on your computer:\nVisual Studio Code Continue extension for Visual Studio Code WSL, if you‚Äôre trying these steps on Windows like me (Ollama isn‚Äôt natively available for Windows yet) I also assume you have some basic level of understanding regarding copilots, Rest APIs, and programming in general in my examples (I‚Äôm not gonna actively explain anything regarding these things).\nI‚Äôve done the following tests on a Dell Latitude 7430 with 16GB of RAM and an Intel i5 1250p. Ollama ran in the WSL-2 environment, limited to 4GB of RAM. Keep in mind that performance may vary depending on your hardware.\nSetting everything up Install Ollama and run the model You should download and set up Ollama on your PC first. Ollama is a neat open-source tool that lets you download and run a variety of LLMs locally. We will use it to host Phi-2.\nThe procedure is simple and is well documented on Ollama‚Äôs website. If you‚Äôre using it on WSL, like me, you can go ahead and run:\n‚ûú ~ curl -fsSL https://ollama.com/install.sh | sh ‚ûú ~ ollama run phi I‚Äôm omitting the output of each command for brevity. The first command installs Ollama on your OS, and the second one automatically downloads the model and runs it interactively. If everything works correctly, you should have an interactive prompt just like this one:\nYou can test the model‚Äôs output right away, such as:\n\u003e\u003e\u003e what is a large language model? A large language model (LLM) is a type of machine learning model that can generate coherent and contextually relevant text based on patterns observed in large volumes of text data. It is designed to understand language at a deeper level by analyzing grammar, semantics, syntax, and the meaning behind words and phrases. LLMs have been trained on massive amounts of text from various sources, such as books, websites, social media posts, and more, allowing them to generate human-like responses in conversation or create original pieces of writing. Now, we have to confirm that Ollama‚Äôs server API is working. I‚Äôm using Insomnia for this step, but you can also proceed with a curl (or Invoke-WebRequest if you‚Äôre using Powershell). I‚Äôm making a POST HTTP request to the /api/chat endpoint as described in Ollama‚Äôs Phi examples documentation. Make sure to include the ‚Äústream‚Äù: false body parameter if you want to get back the complete response (if you don‚Äôt do that, the API answers in a stream-like way, giving you back the first message only)\nAs you can see, the model answered our question. Now it‚Äôs time to set up Continue on Visual Studio Code.\nüí°If you close the interactive prompt, the model will stop responding. To avoid starting it manually every time, consider setting up a systemd service.\nSetup Continue to use Phi-2 on Ollama Assuming you‚Äôve already installed the Continue extension from the marketplace and went through the quickstart tutorial, we can open the settings from the extension‚Äôs icon in VS Code to configure our custom provider: click on the gear icon to open up the config.json file, then, assuming your Ollama server is running on localhost:11434, add the following lines in the ‚Äúmodels‚Äù array:\n{ \"model\": \"phi\", \"title\": \"Ollama-Phi\", \"completionOptions\": {}, \"apiBase\": \"localhost:11434\", \"provider\": \"ollama\" } And that‚Äôs it! We can then select the model from the dropdown in the left corner and start testing it!\nTesting the copilot Now, we‚Äôre interested in testing the capabilities of our Phi-powered copilot. After creating an empty test.py file, we press the ‚Äúctrl+shift+L‚Äù shortcut, and the Quick-edit contextual menu opens up.\nüí°Phi-2 should perform well with Python. Its older brothers Phi-1 and Phi-1.5 were trained with a lot of Python code, examples, and coding exercises, among some other general reasoning-related data.\nIf you want more info on the internals of the training of both older models, you can check here for Phi-1, and here for Phi-1.5.\nTo test the copilot, we provide it with the following prompt: ‚Äúwrite a python app with a framework of your choice that accepts GET requests on a endpoint and answers back with a list of Users. The Users must be stored in a local data structure, avoid adding database related code‚Äù:\nThe copilot came up with the following python code:\nfrom flask import Flask, jsonify, request app = Flask(__name__) users = [{'username': 'user1', 'age': 23}, {'username': 'user2', 'age': 21}] @app.route('/') def index(): return jsonify({'Users': users}) if __name__ == '__main__': app.run(debug=True) I‚Äôve added a couple of newlines for readability, but the code looks good: it fits perfectly the requirements. If we try to run it, we obtain the following error:\nTraceback (most recent call last): File \"C:\\Users\\tommaso.colella\\projects\\testOllama\\test.py\", line 1, in from flask import Flask, jsonify ModuleNotFoundError: No module named 'flask' This is to be expected. VS Code warned us about the missing library on line 1. Let‚Äôs hit the ‚Äúctrl+shift+R‚Äù shortcut to let Continue debug our terminal automatically:\nPhi-2 correctly identified the problem: we‚Äôre missing the Flask web framework. We proceed as suggested and then run the app again:\nPS C:\\Users\\tommaso.colella\\projects\\testOllama\u003e pip install flask # output omitted for brevity PS C:\\Users\\tommaso.colella\\projects\\testOllama\u003e python .\\test.py * Serving Flask app 'test' * Debug mode: on WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Running on http://127.0.0.1:5000 Press CTRL+C to quit * Restarting with stat * Debugger is active! * Debugger PIN: 125-085-252 If we try to curl the provided address, we get:\nPS C:\\Users\\tommaso.colella\\projects\\testOllama\u003e curl http://127.0.0.1:5000 { \"Users\": [ { \"age\": 23, \"username\": \"user1\" }, { \"age\": 21, \"username\": \"user2\" } ] } Nice! Let‚Äôs proceed by asking Phi-2 to add more CRUD functionalities. We highlight the existing code and bring it to the current context, then we provide Continue with the following prompt ‚Äúcan you please add endpoints to add users, delete users, and get a user by its index in the list?‚Äù\nThis is the code it came up with:\nfrom flask import Flask, jsonify, request app = Flask(__name__) users = [{'username': 'user1', 'age': 23}, {'username': 'user2', 'age': 21}] @app.route('/') def index(): return jsonify({'Users': users}) @app.route('/add_user', methods=['POST']) def add_user(): new_user = request.get_json() users.append(new_user) return jsonify({'success': True, 'User': new_user}), 201 @app.route('/delete_user', methods=['POST']) def delete_user(): user_to_delete = request.get_json() for user in users: if user['username'] == user_to_delete['username']: users.remove(user) return jsonify({'success': True, 'User': user}), 200 return jsonify({'error': 'User not found'}), 404 @app.route('/get_user', methods=['GET']) def get_user(): user_index = int(request.args.get('userIndex')) if user_index \u003c len(users): return jsonify({'User': users[user_index]}), 200 else: return jsonify({'error': 'Invalid User Index'}), 404 I think you can see where this is going.\nI don‚Äôt really like the query string argument in the \"/get_user\" endpoint, and the produced code is missing the \"__main__\" entry point, but these are trivial things, and overall code seems great for such a small model running locally in a WSL-2 environment!\nConclusions With small-scale tests like these, we can get a taste of the capabilities of this new SML. There‚Äôs still a lot of work to do in the field of SMLs, and the frontier is wide open: I really hope that these new models can help combat the technocratic mantra of ‚Äúthrow more compute at it‚Äù which seems to be the leading thought in Silicon Valley at the moment. I‚Äôm interested in testing how Phi-2 performs with RAG-based workloads, especially for enterprise chatbots. Time permitting, I might try to do that in a future article.\nReferences Phi-2: The surprising power of small language models - Microsoft Research\n[2306.11644] Textbooks Are All You Need (arxiv.org)\n[2309.05463] Textbooks Are All You Need II: phi-1.5 technical report (arxiv.org)\nOllama\n","wordCount":"1641","inLanguage":"en","image":"https://gioleppe.github.io/images/ollama_phi_cover_cropped.jpg","datePublished":"2024-02-19T00:00:00Z","dateModified":"2024-02-19T00:00:00Z","author":{"@type":"Person","name":"Tommaso Colella"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gioleppe.github.io/posts/diy-copilot-phi/"},"publisher":{"@type":"Organization","name":"Tommaso Colella | SWE","logo":{"@type":"ImageObject","url":"https://gioleppe.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gioleppe.github.io/ accesskey=h title="Tommaso Colella | SWE (Alt + H)">Tommaso Colella | SWE</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://gioleppe.github.io/about title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gioleppe.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://gioleppe.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">DIY self-hosted Copilot using Phi-2 and Ollama</h1><div class=post-meta><span title='2024-02-19 00:00:00 +0000 UTC'>February 19, 2024</span>&nbsp;¬∑&nbsp;8 min&nbsp;¬∑&nbsp;Tommaso Colella</div></header><figure class=entry-cover><img loading=eager src=https://gioleppe.github.io/images/ollama_phi_cover_cropped.jpg alt="A digital art llama with Microsoft's logo"><p>Microsoft lloves llamas</p></figure><div class=post-content><p>On December 12th, <a href=https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/>Microsoft released their latest &ldquo;SML&rdquo; or &ldquo;Small Language Model&rdquo;</a> Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat &ldquo;green&rdquo; model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it&rsquo;s not <strong>THAT</strong> small (it has 2.7 billion parameters), but it is at least 2 to 3 orders of magnitude smaller than the state-of-the-art model GPT4 by OpenAI (public data on the internals of GPT4 is not available, so we can only raise conjectures).</p><blockquote><p>üí°In the last months, interest in SMLs has been rapidly rising. What they really shine on, other than being suitable for deployment on edge devices such as mobile phones is performance per watt. Reducing the carbon footprint for these natural language models (for both the training and inference phases) is increasingly important in a world where companies and policymakers should be very considerate about climate change and actions to combat it.</p></blockquote><p>Being an incurable nerd, I thought about doing some experiments with it, so I set up my plan to use it as a drop-in replacement for Github Copilot, at least to test its performance in a real-world-like scenario such as Python programming.</p><p>I get it. Copilot is so cool and is based on a state-of-the-art model, so why would I replace it with a worse one? Well, there are a couple of good reasons one might use it in place of the bigger and stronger competition:</p><ol><li>As a lightweight model, it can be self-hosted on a standard prosumer PC. No need to pay for loan-shark-priced internet usage next time you&rsquo;re on an international flight and wanna kick up your ai-augmented code editor!</li><li>You are 100% sure your data isn&rsquo;t ending up in a future training dataset. Prompts won&rsquo;t leave your device at all (or local network, as long as you properly self-host the model).</li><li>It&rsquo;s plainly fun to experiment with these things, so why not?</li></ol><p>As you will see, the process is straightforward. To quote (and slightly rephrase) Linus Torvalds, talk is cheap, let&rsquo;s jump straight into the code!</p><hr><h4 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h4><p>For our DIY copilot, we need a couple of tools first, so be sure to have the following installed on your computer:</p><ul><li><a href=https://code.visualstudio.com/>Visual Studio Code</a></li><li><a href=https://continue.dev/docs/quickstart>Continue</a> extension for Visual Studio Code</li><li><a href=https://learn.microsoft.com/en-us/windows/wsl/install>WSL</a>, if you&rsquo;re trying these steps on Windows like me (Ollama isn&rsquo;t natively available for Windows yet)</li></ul><p>I also assume you have some basic level of understanding regarding copilots, Rest APIs, and programming in general in my examples (I&rsquo;m not gonna actively explain anything regarding these things).</p><p>I&rsquo;ve done the following tests on a Dell Latitude 7430 with 16GB of RAM and an Intel i5 1250p. Ollama ran in the WSL-2 environment, limited to 4GB of RAM. Keep in mind that performance may vary depending on your hardware.</p><hr><h4 id=setting-everything-up>Setting everything up<a hidden class=anchor aria-hidden=true href=#setting-everything-up>#</a></h4><h5 id=install-ollama-and-run-the-model>Install Ollama and run the model<a hidden class=anchor aria-hidden=true href=#install-ollama-and-run-the-model>#</a></h5><p>You should download and set up <a href=https://ollama.com/>Ollama</a> on your PC first. Ollama is a neat open-source tool that lets you download and run a variety of LLMs locally. We will use it to host Phi-2.</p><p>The procedure is simple and is well documented on Ollama&rsquo;s website. If you&rsquo;re using it on WSL, like me, you can go ahead and run:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>‚ûú  ~ curl -fsSL https://ollama.com/install.sh <span class=p>|</span> sh 
</span></span><span class=line><span class=cl>‚ûú  ~ ollama run phi
</span></span></code></pre></div><p>I&rsquo;m omitting the output of each command for brevity. The first command installs Ollama on your OS, and the second one automatically downloads the model and runs it interactively. If everything works correctly, you should have an interactive prompt just like this one:</p><p><img loading=lazy src=images/ollama_run_phi.png alt="ollama phi interactive prompt"></p><p>You can test the model&rsquo;s output right away, such as:</p><pre tabindex=0><code>&gt;&gt;&gt; what is a large language model?
 A large language model (LLM) is a type of machine learning model that can generate
coherent and contextually relevant text based on patterns observed in large volumes of
text data. It is designed to understand language at a deeper level by analyzing grammar,
semantics, syntax, and the meaning behind words and phrases. LLMs have been trained on
massive amounts of text from various sources, such as books, websites, social media
posts, and more, allowing them to generate human-like responses in conversation or create
original pieces of writing.
</code></pre><p>Now, we have to confirm that Ollama&rsquo;s server API is working. I&rsquo;m using Insomnia for this step, but you can also proceed with a curl (or Invoke-WebRequest if you&rsquo;re using Powershell). I&rsquo;m making a POST HTTP request to the /api/chat endpoint as described in <a href=https://ollama.com/library/phi>Ollama&rsquo;s Phi examples documentation</a>. Make sure to include the &ldquo;stream&rdquo;: false body parameter if you want to get back the complete response (if you don&rsquo;t do that, the API answers in a stream-like way, giving you back the first message only)</p><p><img loading=lazy src=images/insomnia_phi_test.png alt="testing ollama api with insomnia"></p><p>As you can see, the model answered our question. Now it&rsquo;s time to set up Continue on Visual Studio Code.</p><blockquote><p>üí°If you close the interactive prompt, the model will stop responding. To avoid starting it manually every time, consider setting up a systemd service.</p></blockquote><h5 id=setup-continue-to-use-phi-2-on-ollama>Setup Continue to use Phi-2 on Ollama<a hidden class=anchor aria-hidden=true href=#setup-continue-to-use-phi-2-on-ollama>#</a></h5><p>Assuming you&rsquo;ve already installed the Continue extension from the marketplace and went through the quickstart tutorial, we can open the settings from the extension&rsquo;s icon in VS Code to configure our custom provider: click on the gear icon to open up the config.json file, then, assuming your Ollama server is running on localhost:11434, add the following lines in the &ldquo;models&rdquo; array:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;model&#34;</span><span class=p>:</span> <span class=s2>&#34;phi&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;title&#34;</span><span class=p>:</span> <span class=s2>&#34;Ollama-Phi&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;completionOptions&#34;</span><span class=p>:</span> <span class=p>{},</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;apiBase&#34;</span><span class=p>:</span> <span class=s2>&#34;localhost:11434&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=nt>&#34;provider&#34;</span><span class=p>:</span> <span class=s2>&#34;ollama&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span></code></pre></div><p>And that&rsquo;s it! We can then select the model from the dropdown in the left corner and start testing it!</p><p><img loading=lazy src=images/continue_model_selector.png alt="selecting phi on continue"></p><h3 id=testing-the-copilot>Testing the copilot<a hidden class=anchor aria-hidden=true href=#testing-the-copilot>#</a></h3><p>Now, we&rsquo;re interested in testing the capabilities of our Phi-powered copilot. After creating an empty test.py file, we press the &ldquo;ctrl+shift+L&rdquo; shortcut, and the Quick-edit contextual menu opens up.</p><blockquote><p>üí°Phi-2 should perform well with Python. Its older brothers Phi-1 and Phi-1.5 were trained with a lot of Python code, examples, and coding exercises, among some other general reasoning-related data.</p><p>If you want more info on the internals of the training of both older models, you can check <a href=https://arxiv.org/abs/2306.11644>here</a> for Phi-1, and <a href=https://arxiv.org/abs/2309.05463>here</a> for Phi-1.5.</p></blockquote><p>To test the copilot, we provide it with the following prompt: <em>&ldquo;write a python app with a framework of your choice that accepts GET requests on a endpoint and answers back with a list of Users. The Users must be stored in a local data structure, avoid adding database related code&rdquo;</em>:</p><p><img loading=lazy src=images/continue_phi_create_endpoint.gif alt="creating an api using continue"></p><p>The copilot came up with the following python code:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>flask</span> <span class=kn>import</span> <span class=n>Flask</span><span class=p>,</span> <span class=n>jsonify</span><span class=p>,</span> <span class=n>request</span>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>Flask</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>users</span> <span class=o>=</span> <span class=p>[{</span><span class=s1>&#39;username&#39;</span><span class=p>:</span> <span class=s1>&#39;user1&#39;</span><span class=p>,</span> <span class=s1>&#39;age&#39;</span><span class=p>:</span> <span class=mi>23</span><span class=p>},</span> 
</span></span><span class=line><span class=cl>       <span class=p>{</span><span class=s1>&#39;username&#39;</span><span class=p>:</span> <span class=s1>&#39;user2&#39;</span><span class=p>,</span> <span class=s1>&#39;age&#39;</span><span class=p>:</span> <span class=mi>21</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s1>&#39;/&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>index</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jsonify</span><span class=p>({</span><span class=s1>&#39;Users&#39;</span><span class=p>:</span> <span class=n>users</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>app</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>debug</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><p>I&rsquo;ve added a couple of newlines for readability, but the code looks good: it fits perfectly the requirements.
If we try to run it, we obtain the following error:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>Traceback <span class=o>(</span>most recent call last<span class=o>)</span>:
</span></span><span class=line><span class=cl>  File <span class=s2>&#34;C:\Users\tommaso.colella\projects\testOllama\test.py&#34;</span>, line 1, in &lt;module&gt;
</span></span><span class=line><span class=cl>    from flask import Flask, jsonify
</span></span><span class=line><span class=cl>ModuleNotFoundError: No module named <span class=s1>&#39;flask&#39;</span>
</span></span></code></pre></div><p>This is to be expected. VS Code warned us about the missing library on line 1. Let&rsquo;s hit the &ldquo;ctrl+shift+R&rdquo; shortcut to let Continue debug our terminal automatically:</p><p><img loading=lazy src=images/continue_phi_debugger.png alt="debugging using continue"></p><p>Phi-2 correctly identified the problem: we&rsquo;re missing the Flask web framework. We proceed as suggested and then run the app again:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>PS </span><span class=n>C:</span><span class=p>\</span><span class=n>Users</span><span class=p>\</span><span class=n>tommaso</span><span class=p>.</span><span class=n>colella</span><span class=p>\</span><span class=n>projects</span><span class=p>\</span><span class=n>testOllama</span><span class=p>&gt;</span> <span class=n>pip</span> <span class=n>install</span> <span class=n>flask</span> 
</span></span><span class=line><span class=cl><span class=c># output omitted for brevity</span>
</span></span><span class=line><span class=cl><span class=nb>PS </span><span class=n>C:</span><span class=p>\</span><span class=n>Users</span><span class=p>\</span><span class=n>tommaso</span><span class=p>.</span><span class=n>colella</span><span class=p>\</span><span class=n>projects</span><span class=p>\</span><span class=n>testOllama</span><span class=p>&gt;</span> <span class=n>python</span> <span class=p>.\</span><span class=n>test</span><span class=p>.</span><span class=py>py</span> 
</span></span><span class=line><span class=cl> <span class=p>*</span> <span class=n>Serving</span> <span class=n>Flask</span> <span class=n>app</span> <span class=s1>&#39;test&#39;</span>
</span></span><span class=line><span class=cl> <span class=p>*</span> <span class=n>Debug</span> <span class=n>mode</span><span class=err>:</span> <span class=n>on</span>
</span></span><span class=line><span class=cl><span class=n>WARNING</span><span class=err>:</span> <span class=n>This</span> <span class=n>is</span> <span class=n>a</span> <span class=n>development</span> <span class=n>server</span><span class=p>.</span> <span class=k>Do</span> <span class=n>not</span> <span class=n>use</span> <span class=n>it</span> <span class=k>in</span> <span class=n>a</span> <span class=n>production</span> <span class=n>deployment</span><span class=p>.</span> <span class=n>Use</span> <span class=n>a</span> <span class=n>production</span> <span class=n>WSGI</span> <span class=n>server</span> <span class=n>instead</span><span class=p>.</span>
</span></span><span class=line><span class=cl> <span class=p>*</span> <span class=n>Running</span> <span class=n>on</span> <span class=n>http</span><span class=err>:</span><span class=p>//</span><span class=mf>127.0</span><span class=p>.</span><span class=py>0</span><span class=p>.</span><span class=mf>1</span><span class=err>:</span><span class=mf>5000</span>
</span></span><span class=line><span class=cl><span class=n>Press</span> <span class=n>CTRL</span><span class=p>+</span><span class=n>C</span> <span class=n>to</span> <span class=n>quit</span>
</span></span><span class=line><span class=cl> <span class=p>*</span> <span class=n>Restarting</span> <span class=n>with</span> <span class=n>stat</span>
</span></span><span class=line><span class=cl> <span class=p>*</span> <span class=n>Debugger</span> <span class=n>is</span> <span class=n>active</span><span class=p>!</span>
</span></span><span class=line><span class=cl> <span class=p>*</span> <span class=n>Debugger</span> <span class=n>PIN</span><span class=err>:</span> <span class=mf>125</span><span class=p>-</span><span class=mf>085</span><span class=p>-</span><span class=mf>252</span>
</span></span></code></pre></div><p>If we try to curl the provided address, we get:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-powershell data-lang=powershell><span class=line><span class=cl><span class=nb>PS </span><span class=n>C:</span><span class=p>\</span><span class=n>Users</span><span class=p>\</span><span class=n>tommaso</span><span class=p>.</span><span class=n>colella</span><span class=p>\</span><span class=n>projects</span><span class=p>\</span><span class=n>testOllama</span><span class=p>&gt;</span> <span class=nb>curl </span><span class=n>http</span><span class=err>:</span><span class=p>//</span><span class=mf>127.0</span><span class=p>.</span><span class=py>0</span><span class=p>.</span><span class=mf>1</span><span class=err>:</span><span class=mf>5000</span>
</span></span><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;Users&#34;</span><span class=err>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=s2>&#34;age&#34;</span><span class=err>:</span> <span class=mf>23</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=s2>&#34;username&#34;</span><span class=err>:</span> <span class=s2>&#34;user1&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=s2>&#34;age&#34;</span><span class=err>:</span> <span class=mf>21</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=s2>&#34;username&#34;</span><span class=err>:</span> <span class=s2>&#34;user2&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>Nice! Let&rsquo;s proceed by asking Phi-2 to add more CRUD functionalities. We highlight the existing code and bring it to the current context, then we provide Continue with the following prompt <em>&ldquo;can you please add endpoints to add users, delete users, and get a user by its index in the list?&rdquo;</em></p><p><img loading=lazy src=images/continue_phi_add_CRUD.png alt="adding more CRUD endpoints"></p><p>This is the code it came up with:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>flask</span> <span class=kn>import</span> <span class=n>Flask</span><span class=p>,</span> <span class=n>jsonify</span><span class=p>,</span> <span class=n>request</span>
</span></span><span class=line><span class=cl><span class=n>app</span> <span class=o>=</span> <span class=n>Flask</span><span class=p>(</span><span class=vm>__name__</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>users</span> <span class=o>=</span> <span class=p>[{</span><span class=s1>&#39;username&#39;</span><span class=p>:</span> <span class=s1>&#39;user1&#39;</span><span class=p>,</span> <span class=s1>&#39;age&#39;</span><span class=p>:</span> <span class=mi>23</span><span class=p>},</span> 
</span></span><span class=line><span class=cl>   <span class=p>{</span><span class=s1>&#39;username&#39;</span><span class=p>:</span> <span class=s1>&#39;user2&#39;</span><span class=p>,</span> <span class=s1>&#39;age&#39;</span><span class=p>:</span> <span class=mi>21</span><span class=p>}]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s1>&#39;/&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>index</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jsonify</span><span class=p>({</span><span class=s1>&#39;Users&#39;</span><span class=p>:</span> <span class=n>users</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s1>&#39;/add_user&#39;</span><span class=p>,</span> <span class=n>methods</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;POST&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>add_user</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>new_user</span> <span class=o>=</span> <span class=n>request</span><span class=o>.</span><span class=n>get_json</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>users</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>new_user</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jsonify</span><span class=p>({</span><span class=s1>&#39;success&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;User&#39;</span><span class=p>:</span> <span class=n>new_user</span><span class=p>}),</span> <span class=mi>201</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s1>&#39;/delete_user&#39;</span><span class=p>,</span> <span class=n>methods</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;POST&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>delete_user</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>user_to_delete</span> <span class=o>=</span> <span class=n>request</span><span class=o>.</span><span class=n>get_json</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>user</span> <span class=ow>in</span> <span class=n>users</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>user</span><span class=p>[</span><span class=s1>&#39;username&#39;</span><span class=p>]</span> <span class=o>==</span> <span class=n>user_to_delete</span><span class=p>[</span><span class=s1>&#39;username&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=n>users</span><span class=o>.</span><span class=n>remove</span><span class=p>(</span><span class=n>user</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>jsonify</span><span class=p>({</span><span class=s1>&#39;success&#39;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span> <span class=s1>&#39;User&#39;</span><span class=p>:</span> <span class=n>user</span><span class=p>}),</span> <span class=mi>200</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>jsonify</span><span class=p>({</span><span class=s1>&#39;error&#39;</span><span class=p>:</span> <span class=s1>&#39;User not found&#39;</span><span class=p>}),</span> <span class=mi>404</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@app.route</span><span class=p>(</span><span class=s1>&#39;/get_user&#39;</span><span class=p>,</span> <span class=n>methods</span><span class=o>=</span><span class=p>[</span><span class=s1>&#39;GET&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_user</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>user_index</span> <span class=o>=</span> <span class=nb>int</span><span class=p>(</span><span class=n>request</span><span class=o>.</span><span class=n>args</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;userIndex&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>user_index</span> <span class=o>&lt;</span> <span class=nb>len</span><span class=p>(</span><span class=n>users</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>jsonify</span><span class=p>({</span><span class=s1>&#39;User&#39;</span><span class=p>:</span> <span class=n>users</span><span class=p>[</span><span class=n>user_index</span><span class=p>]}),</span> <span class=mi>200</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>jsonify</span><span class=p>({</span><span class=s1>&#39;error&#39;</span><span class=p>:</span> <span class=s1>&#39;Invalid User Index&#39;</span><span class=p>}),</span> <span class=mi>404</span>
</span></span></code></pre></div><p>I think you can see where this is going.</p><p>I don&rsquo;t really like the query string argument in the <em>"/get_user"</em> endpoint, and the produced code is missing the <em>"__main__"</em> entry point, but these are trivial things, and overall code seems great for such a small model running locally in a WSL-2 environment!</p><h3 id=conclusions>Conclusions<a hidden class=anchor aria-hidden=true href=#conclusions>#</a></h3><p>With small-scale tests like these, we can get a taste of the capabilities of this new SML. There&rsquo;s still a lot of work to do in the field of SMLs, and the frontier is wide open: I really hope that these new models can help combat the technocratic mantra of &ldquo;throw more compute at it&rdquo; which seems to be the leading thought in Silicon Valley at the moment. I&rsquo;m interested in testing how Phi-2 performs with RAG-based workloads, especially for enterprise chatbots. Time permitting, I might try to do that in a future article.</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p><a href=https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/>Phi-2: The surprising power of small language models - Microsoft Research</a></p><p>[<a href=https://arxiv.org/abs/2306.11644>2306.11644] Textbooks Are All You Need (arxiv.org)</a></p><p>[<a href=https://arxiv.org/abs/2309.05463>2309.05463] Textbooks Are All You Need II: phi-1.5 technical report (arxiv.org)</a></p><p><a href=https://ollama.com/>Ollama</a></p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=https://gioleppe.github.io/posts/gemma-copilot/><span class=title>¬´ Prev</span><br><span>Self-hosted Copilot using Google Gemma and Ollama (+ Phi-2 comparison)</span>
</a><a class=next href=https://gioleppe.github.io/posts/azure-openai-service-opentofu/><span class=title>Next ¬ª</span><br><span>Deploying AzureOpenAI Service using OpenTofu</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://gioleppe.github.io/>Tommaso Colella | SWE</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>