<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deploying AzureOpenAI Service using OpenTofu | Tommaso Colella | SWE</title>
<meta name=keywords content><meta name=description content="Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.
I&rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we&rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I&rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi&rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties."><meta name=author content="Tommaso Colella"><link rel=canonical href=https://gioleppe.github.io/posts/azure-openai-opentofu/><meta name=google-site-verification content="G-JHDYWZPGQF"><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://gioleppe.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://gioleppe.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://gioleppe.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://gioleppe.github.io/apple-touch-icon.png><link rel=mask-icon href=https://gioleppe.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/ld+json>{"@context":"https://schema.org/","@type":"Person","name":"Tommaso Colella","url":"https://gioleppe.github.io/","image":"https://media.licdn.com/dms/image/C5603AQEo-0VsShX7eg/profile-displayphoto-shrink_400_400/0/1612869300096?e=1676505600&v=beta&t=C0ZDSUriW6h0wK3Py6DoZ8viruvCLJdTg99jQCGsl9Y","sameAs":["https://www.linkedin.com/in/tommaso-colella-051012143/","https://github.com/gioleppe","https://www.facebook.com/amuchina"],"jobTitle":"Software Engineer","worksFor":{"@type":"Organization","name":"YOOX Net-a-Porter Group S.p.A"}}</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-JHDYWZPGQF"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JHDYWZPGQF",{anonymize_ip:!1})}</script><meta property="og:title" content="Deploying AzureOpenAI Service using OpenTofu"><meta property="og:description" content="Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.
I&rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we&rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I&rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi&rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties."><meta property="og:type" content="article"><meta property="og:url" content="https://gioleppe.github.io/posts/azure-openai-opentofu/"><meta property="og:image" content="https://gioleppe.github.io/images/azure-openai-article-cover.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-14T00:00:00+00:00"><meta property="og:site_name" content="Tommaso Colella"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://gioleppe.github.io/images/azure-openai-article-cover.jpg"><meta name=twitter:title content="Deploying AzureOpenAI Service using OpenTofu"><meta name=twitter:description content="Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.
I&rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we&rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I&rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi&rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://gioleppe.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Deploying AzureOpenAI Service using OpenTofu","item":"https://gioleppe.github.io/posts/azure-openai-opentofu/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deploying AzureOpenAI Service using OpenTofu","name":"Deploying AzureOpenAI Service using OpenTofu","description":"Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.\nI\u0026rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we\u0026rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I\u0026rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi\u0026rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties.","keywords":[],"articleBody":"Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.\nI‚Äôm lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we‚Äôre always looking for new ways to bring value to the market using cutting-edge services. For this reason, I‚Äôve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi‚Äôs most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties.\nIn the last year, I‚Äôve used OpenTofu/Terraform daily to provision our infrastructure. I‚Äôve really come to appreciate the advantages of Infrastructure as Code, some of which are:\nEliminating error-prone manual operations Easy reproducibility of configurations for different clients Explaining the capabilities of such a tool in a few paragraphs is nearly impossible, but these are some reasons why I chose it over other deployment methods.\nCombining business with pleasure, I tried to find the best way to automatize the deployment of AzureOpenAi services using OpenTofu/Terraform. The following is a brief recount of what I did.\nPrerequisites Before trying any of the following:\nObtain an Azure subscription with access to AzureOpenAi Download OpenTofu(or Terraform) and be sure to know how to authenticate to your Azure subscription using one of the methods listed on top of this page Download Chatbox or any other LLM frontend to test the deployed model (optional) From now on, I also assume you have some basic understanding of Azure, IaC, and LLMs, at least on a surface level.\nüí° Why OpenTofu? Some months back, Hashicorp changed the license of Terraform to the BUSL. Their action was very controversial. That encouraged many organizations to reconsider their usage of the tool.I‚Äôm interested in supporting open-source and free software when having a choice: that‚Äôs why I‚Äôm using OpenTofu. The examples in this article are compatible with Terraform nonetheless.\nChoosing the right way I‚Äôve already been experimenting with AzureOpenAi for a while. Moreover, I‚Äôm using Terraform daily at work, so the first thing I did was look at the docs of the Azure Terraform provider. I found there is a resource to deploy Cognitive Services in it but no direct way to deploy the AzureOpenAi Service itself.\nBeing lazy, I didn‚Äôt want to write all the configurations myself. I instantly hit Google and quickly found this neat module, backed by Microsoft and Hashicorp and developed by community contributors. At the moment, this seems like the fastest way to deploy an OpenAi model on Azure.\nThe module is not well known: when writing this article, on Feb 14th, 2024, the public registry showed only 6.4k downloads since May 2023, when it was first published, so I tried it and wrote the following OpenTofu code to check the module‚Äôs maturity and usability.\nPlease be aware that the following examples have been run in a local environment, so this code is not ready for production deployment.\n// everything in a single file for brevity terraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"~\u003e3.80.0\" // most recent version supported by the module } // required by the module modtm = { source = \"Azure/modtm\" version = \"\u003e= 0.1.8, \u003c 1.0\" } } } provider \"azurerm\" { features {} } provider \"modtm\" { enabled = false } // be sure to select a location where AzureOpenAi // and the model/model version are supported resource \"azurerm_resource_group\" \"openai_deployment_test\" { name = \"rg-oai-dev\" location = \"France Central\" } module \"openai\" { source = \"Azure/openai/azurerm\" version = \"0.1.3\" resource_group_name = azurerm_resource_group.openai_deployment_test.name location = azurerm_resource_group.openai_deployment_test.location public_network_access_enabled = true deployment = { \"gpt-4-turbo\" = { name = \"gpt-4\" model_format = \"OpenAI\" model_name = \"gpt-4\" model_version = \"1106-Preview\" scale_type = \"Standard\" capacity = 10 // rate limited to 10k tokens per minute }, } depends_on = [ azurerm_resource_group.openai_deployment_test ] } // outputs added for next step convenience output \"endpoint\" { value = module.openai.openai_endpoint } output \"primary_key\" { value = module.openai.primary_key } I assume you are already authenticated to your Azure subscription (perhaps using the Azure CLI as suggested in the Azure provider documentation).\nAfter running a tofu init and a tofu plan, everything looks fine: the IaC tool wants to create the resources needed to make our deployment work.\nOpenTofu will perform the following actions: # azurerm_resource_group.openai_deployment_test will be created + resource \"azurerm_resource_group\" \"openai_deployment_test\" { + id = (known after apply) + location = \"francecentral\" + name = \"rg-oai-dev\" } # module.openai.data.azurerm_resource_group.this will be read during apply # (depends on a resource or a module with changes pending) \u003c= data \"azurerm_resource_group\" \"this\" { + id = (known after apply) + location = (known after apply) ... (omitted for brevity) ... Plan: 5 to add, 0 to change, 0 to destroy. Changes to Outputs: + endpoint = (known after apply) + primary_key = (sensitive value) After proceeding with tofu apply and witnessing success, we can check if the resources have been provisioned correctly on our subscription.\nIt seems everything worked correctly. Azure OpenAI Service generates a public endpoint and a primary key that we can use together to test our deployment.\nTesting the deployment Chatbox is a neat desktop client that supports various LLMs. I‚Äôm assuming you‚Äôve downloaded and installed it in the following steps.\nFirst, we need to retrieve our deployment‚Äôs endpoint and primary key: we can do so by using tofu output, having defined proper output blocks in the infrastructure code above.\n‚ûú test-openai-automatic-provisioning tofu output endpoint \"https://azure-openai-318516.openai.azure.com/\" ‚ûú test-openai-automatic-provisioning tofu output primary_key \"853bb4c221754e158f431d5591bf3a6c\" Be sure that the endpoint and the key will be long gone by the time I publish this!\nWe open the app and configure our deployment by going to the Settings tab. Check the ‚ÄúModel \u0026 Token‚Äù subsection if you want to manage model temperature and max generated tokens.\nAnd that‚Äôs it! You can now use Chatbox (maybe with a nice Persona prompting pattern) to test your deployment. It also supports some neat features such as export to Markdown, if you need them.\nSecurity The module also supports some enterprise-grade security measures. Notably, it features, among other things, Private Endpoint and Access Control List integrations.\nThese security features go beyond the proposed scope of this technical article. Even then, an architect should consider them when developing a secure enterprise-grade solution for customers demanding extra safety over their deployments.\nConclusions In this small technical article, I tried to show how to provision an ‚ÄúAzureOpenAi Service‚Äù resource by using a widely adopted IaC tool such as OpenTofu/Terraform. The agility of such tools makes it extremely simple and fast to bring value to the customers, and, together with DevOps practices, helps developers and architects focus on solving real problems.\nSome next steps could be trying to automate the fine-tuning of the model or automatically benchmarking the deployment against a set of client-business-related tasks. I might try to do some of that in a future blog post.\nThanks for coming this far. Power to the nerds!\nReferences Accelerating AI adoption on Azure with Terraform (hashicorp.com)\nGitHub - Azure/terraform-azurerm-openai: Terraform module for deploying Azure OpenAI Service.\nOpenTofu\nLimited access to Azure OpenAI Service - Azure AI services | Microsoft Learn\nAzure Provider: Authenticating via the Azure CLI | Guides | hashicorp/azurerm | Terraform | Terraform Registry\nGitHub - Bin-Huang/chatbox: Chatbox is a desktop client for ChatGPT, Claude and other LLMs, available on Windows, Mac, Linux\nazurerm_cognitive_deployment | Resources | hashicorp/azurerm | Terraform | Terraform Registry\n","wordCount":"1253","inLanguage":"en","image":"https://gioleppe.github.io/images/azure-openai-article-cover.jpg","datePublished":"2024-02-14T00:00:00Z","dateModified":"2024-02-14T00:00:00Z","author":{"@type":"Person","name":"Tommaso Colella"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://gioleppe.github.io/posts/azure-openai-opentofu/"},"publisher":{"@type":"Organization","name":"Tommaso Colella | SWE","logo":{"@type":"ImageObject","url":"https://gioleppe.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://gioleppe.github.io/ accesskey=h title="Tommaso Colella | SWE (Alt + H)">Tommaso Colella | SWE</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://gioleppe.github.io/about title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://gioleppe.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://gioleppe.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Deploying AzureOpenAI Service using OpenTofu</h1><div class=post-meta><span title='2024-02-14 00:00:00 +0000 UTC'>February 14, 2024</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;Tommaso Colella</div></header><figure class=entry-cover><img loading=eager src=https://gioleppe.github.io/images/azure-openai-article-cover.jpg alt="A lightbulb in a cloud with the Azure logo in it"><p>Bing Chat couldn&rsquo;t get the logo right</p></figure><div class=post-content><p>Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.</p><p>I&rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we&rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I&rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi&rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties.</p><p>In the last year, I&rsquo;ve used OpenTofu/Terraform daily to provision our infrastructure. I&rsquo;ve really come to appreciate the advantages of Infrastructure as Code, some of which are:</p><ul><li>Eliminating error-prone manual operations</li><li>Easy reproducibility of configurations for different clients</li></ul><p>Explaining the capabilities of such a tool in a few paragraphs is nearly impossible, but these are some reasons why I chose it over other deployment methods.</p><p>Combining business with pleasure, I tried to find the best way to automatize the deployment of AzureOpenAi services using OpenTofu/Terraform. The following is a brief recount of what I did.</p><h4 id=prerequisites>Prerequisites<a hidden class=anchor aria-hidden=true href=#prerequisites>#</a></h4><p>Before trying any of the following:</p><ul><li>Obtain an Azure subscription with <a href=https://learn.microsoft.com/en-us/legal/cognitive-services/openai/limited-access#registration-process>access to AzureOpenAi</a></li><li>Download <a href=https://opentofu.org/>OpenTofu</a>(or Terraform) and be sure to know how to authenticate to your Azure subscription using one of the methods listed on top of <a href=https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/azure_cli>this</a> page</li><li>Download <a href=https://github.com/Bin-Huang/chatbox/tree/main>Chatbox</a> or any other LLM frontend to test the deployed model (optional)</li></ul><p>From now on, I also assume you have some basic understanding of Azure, IaC, and LLMs, at least on a surface level.</p><blockquote><p>üí° <strong>Why OpenTofu?</strong> Some months back, Hashicorp changed the license of Terraform to the BUSL. Their action was very controversial. That encouraged many organizations to reconsider their usage of the tool.I&rsquo;m interested in supporting open-source and free software when having a choice: that&rsquo;s why I&rsquo;m using OpenTofu. The examples in this article are compatible with Terraform nonetheless.</p></blockquote><h3 id=choosing-the-right-way>Choosing the right way<a hidden class=anchor aria-hidden=true href=#choosing-the-right-way>#</a></h3><p>I&rsquo;ve already been experimenting with AzureOpenAi for a while. Moreover, I&rsquo;m using Terraform daily at work, so the first thing I did was look at the docs of the Azure Terraform provider. I found there is a <a href=https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/cognitive_deployment>resource to deploy Cognitive Services</a> in it but no direct way to deploy the AzureOpenAi Service itself.</p><p>Being lazy, I didn&rsquo;t want to write all the configurations myself. I instantly hit Google and quickly found <a href=https://github.com/Azure/terraform-azurerm-openai/tree/main>this</a> neat module, <a href=https://www.hashicorp.com/blog/accelerating-ai-adoption-on-azure-with-terraform>backed</a> by Microsoft and Hashicorp and developed by community contributors. At the moment, this seems like the fastest way to deploy an OpenAi model on Azure.</p><p>The module is not well known: when writing this article, on Feb 14th, 2024, the <a href=https://registry.terraform.io/modules/Azure/openai/azurerm/0.1.3>public registry</a> showed only 6.4k downloads since May 2023, when it was first published, so I tried it and wrote the following OpenTofu code to check the module&rsquo;s maturity and usability.</p><p>Please be aware that the following examples have been run in a local environment, so this code is not ready for production deployment.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-terraform data-lang=terraform><span class=line><span class=cl><span class=c1>// everything in a single file for brevity
</span></span></span><span class=line><span class=cl><span class=c1></span>
</span></span><span class=line><span class=cl><span class=nx>terraform</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>required_providers</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=na>azurerm</span> = <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=na>source</span>  = <span class=s2>&#34;hashicorp/azurerm&#34;</span>
</span></span><span class=line><span class=cl>      <span class=na>version</span> = <span class=s2>&#34;~&gt;3.80.0&#34;</span><span class=c1> // most recent version supported by the module
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>}</span><span class=c1>
</span></span></span><span class=line><span class=cl><span class=c1>
</span></span></span><span class=line><span class=cl><span class=c1>    // required by the module
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=na>modtm</span> = <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=na>source</span>  = <span class=s2>&#34;Azure/modtm&#34;</span>
</span></span><span class=line><span class=cl>      <span class=na>version</span> = <span class=s2>&#34;&gt;= 0.1.8, &lt; 1.0&#34;</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>
</span></span></span><span class=line><span class=cl><span class=kr>provider</span> <span class=s2>&#34;azurerm&#34;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nx>features</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>
</span></span></span><span class=line><span class=cl><span class=kr>provider</span> <span class=s2>&#34;modtm&#34;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=na>enabled</span> = <span class=kc>false</span>
</span></span><span class=line><span class=cl><span class=p>}</span><span class=c1>
</span></span></span><span class=line><span class=cl><span class=c1>
</span></span></span><span class=line><span class=cl><span class=c1>// be sure to select a location where AzureOpenAi
</span></span></span><span class=line><span class=cl><span class=c1>// and the model/model version are supported
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kr>resource</span> <span class=s2>&#34;azurerm_resource_group&#34;</span> <span class=s2>&#34;openai_deployment_test&#34;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=na>name</span>     = <span class=s2>&#34;rg-oai-dev&#34;</span>
</span></span><span class=line><span class=cl>  <span class=na>location</span> = <span class=s2>&#34;France Central&#34;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>
</span></span></span><span class=line><span class=cl><span class=kr>module</span> <span class=s2>&#34;openai&#34;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=na>source</span>                        = <span class=s2>&#34;Azure/openai/azurerm&#34;</span>
</span></span><span class=line><span class=cl>  <span class=na>version</span>                       = <span class=s2>&#34;0.1.3&#34;</span>
</span></span><span class=line><span class=cl>  <span class=na>resource_group_name</span>           = <span class=nx>azurerm_resource_group</span><span class=p>.</span><span class=nx>openai_deployment_test</span><span class=p>.</span><span class=nx>name</span>
</span></span><span class=line><span class=cl>  <span class=na>location</span>                      = <span class=nx>azurerm_resource_group</span><span class=p>.</span><span class=nx>openai_deployment_test</span><span class=p>.</span><span class=nx>location</span>
</span></span><span class=line><span class=cl>  <span class=na>public_network_access_enabled</span> = <span class=kc>true</span>
</span></span><span class=line><span class=cl>  <span class=na>deployment</span> = <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;gpt-4-turbo&#34;</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>      <span class=na>name</span>          = <span class=s2>&#34;gpt-4&#34;</span>
</span></span><span class=line><span class=cl>      <span class=na>model_format</span>  = <span class=s2>&#34;OpenAI&#34;</span>
</span></span><span class=line><span class=cl>      <span class=na>model_name</span>    = <span class=s2>&#34;gpt-4&#34;</span>
</span></span><span class=line><span class=cl>      <span class=na>model_version</span> = <span class=s2>&#34;1106-Preview&#34;</span>
</span></span><span class=line><span class=cl>      <span class=na>scale_type</span>    = <span class=s2>&#34;Standard&#34;</span>
</span></span><span class=line><span class=cl>      <span class=na>capacity</span>      = <span class=m>10</span><span class=c1> // rate limited to 10k tokens per minute
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=p>},</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl>  <span class=na>depends_on</span> = <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=nx>azurerm_resource_group</span><span class=p>.</span><span class=nx>openai_deployment_test</span>
</span></span><span class=line><span class=cl>  <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=p>}</span><span class=c1>
</span></span></span><span class=line><span class=cl><span class=c1>
</span></span></span><span class=line><span class=cl><span class=c1>// outputs added for next step convenience
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kr>output</span> <span class=s2>&#34;endpoint&#34;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=na>value</span> = <span class=nb>module</span><span class=p>.</span><span class=nx>openai</span><span class=p>.</span><span class=nx>openai_endpoint</span>  
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=kr>
</span></span></span><span class=line><span class=cl><span class=kr>output</span> <span class=s2>&#34;primary_key&#34;</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=na>value</span> = <span class=nb>module</span><span class=p>.</span><span class=nx>openai</span><span class=p>.</span><span class=nx>primary_key</span>     
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>I assume you are already authenticated to your Azure subscription (perhaps using the Azure CLI as suggested in the Azure provider documentation).</p><p>After running a <code>tofu init</code> and a <code>tofu plan</code>, everything looks fine: the IaC tool wants to create the resources needed to make our deployment work.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>OpenTofu will perform the following actions:
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># azurerm_resource_group.openai_deployment_test will be created</span>
</span></span><span class=line><span class=cl>  + resource <span class=s2>&#34;azurerm_resource_group&#34;</span> <span class=s2>&#34;openai_deployment_test&#34;</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>      + <span class=nv>id</span>       <span class=o>=</span> <span class=o>(</span>known after apply<span class=o>)</span>
</span></span><span class=line><span class=cl>      + <span class=nv>location</span> <span class=o>=</span> <span class=s2>&#34;francecentral&#34;</span>
</span></span><span class=line><span class=cl>      + <span class=nv>name</span>     <span class=o>=</span> <span class=s2>&#34;rg-oai-dev&#34;</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>  <span class=c1># module.openai.data.azurerm_resource_group.this will be read during apply</span>
</span></span><span class=line><span class=cl>  <span class=c1># (depends on a resource or a module with changes pending)</span>
</span></span><span class=line><span class=cl> &lt;<span class=o>=</span> data <span class=s2>&#34;azurerm_resource_group&#34;</span> <span class=s2>&#34;this&#34;</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>      + <span class=nv>id</span>         <span class=o>=</span> <span class=o>(</span>known after apply<span class=o>)</span>
</span></span><span class=line><span class=cl>      + <span class=nv>location</span>   <span class=o>=</span> <span class=o>(</span>known after apply<span class=o>)</span>
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl><span class=o>(</span>omitted <span class=k>for</span> brevity<span class=o>)</span>
</span></span><span class=line><span class=cl>...
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Plan: <span class=m>5</span> to add, <span class=m>0</span> to change, <span class=m>0</span> to destroy.
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Changes to Outputs:
</span></span><span class=line><span class=cl>  + <span class=nv>endpoint</span>    <span class=o>=</span> <span class=o>(</span>known after apply<span class=o>)</span>
</span></span><span class=line><span class=cl>  + <span class=nv>primary_key</span> <span class=o>=</span> <span class=o>(</span>sensitive value<span class=o>)</span>
</span></span></code></pre></div><p>After proceeding with <code>tofu apply</code> and witnessing success, we can check if the resources have been provisioned correctly on our subscription.</p><p><img loading=lazy src=images/successful_deployment.png alt=image-20240213190159009></p><p>It seems everything worked correctly. Azure OpenAI Service generates a public endpoint and a primary key that we can use together to test our deployment.</p><h3 id=testing-the-deployment>Testing the deployment<a hidden class=anchor aria-hidden=true href=#testing-the-deployment>#</a></h3><p>Chatbox is a neat desktop client that supports various LLMs. I&rsquo;m assuming you&rsquo;ve downloaded and installed it in the following steps.</p><p>First, we need to retrieve our deployment&rsquo;s endpoint and primary key: we can do so by using tofu output, having defined proper output blocks in the infrastructure code above.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>‚ûú  test-openai-automatic-provisioning tofu output endpoint
</span></span><span class=line><span class=cl><span class=s2>&#34;https://azure-openai-318516.openai.azure.com/&#34;</span>
</span></span><span class=line><span class=cl>‚ûú  test-openai-automatic-provisioning tofu output primary_key
</span></span><span class=line><span class=cl><span class=s2>&#34;853bb4c221754e158f431d5591bf3a6c&#34;</span>
</span></span></code></pre></div><p>Be sure that the endpoint and the key will be long gone by the time I publish this!</p><p>We open the app and configure our deployment by going to the Settings tab. Check the &ldquo;Model & Token&rdquo; subsection if you want to manage model temperature and max generated tokens.</p><p><img loading=lazy src=images/chatbox_settings.png alt=image-20240213191252951></p><p>And that&rsquo;s it! You can now use Chatbox (maybe with a nice Persona prompting pattern) to test your deployment. It also supports some neat features such as export to Markdown, if you need them.</p><p><img loading=lazy src=images/chatbox_chat.png alt=image-20240213191709114></p><h3 id=security>Security<a hidden class=anchor aria-hidden=true href=#security>#</a></h3><p>The module also supports some enterprise-grade security measures. Notably, it features, among other things, Private Endpoint and Access Control List integrations.</p><p>These security features go beyond the proposed scope of this technical article. Even then, an architect should consider them when developing a secure enterprise-grade solution for customers demanding extra safety over their deployments.</p><h3 id=conclusions>Conclusions<a hidden class=anchor aria-hidden=true href=#conclusions>#</a></h3><p>In this small technical article, I tried to show how to provision an &ldquo;AzureOpenAi Service&rdquo; resource by using a widely adopted IaC tool such as OpenTofu/Terraform. The agility of such tools makes it extremely simple and fast to bring value to the customers, and, together with DevOps practices, helps developers and architects focus on solving real problems.</p><p>Some next steps could be trying to automate the fine-tuning of the model or automatically benchmarking the deployment against a set of client-business-related tasks. I might try to do some of that in a future blog post.</p><p>Thanks for coming this far. Power to the nerds!</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><p><a href=https://www.hashicorp.com/blog/accelerating-ai-adoption-on-azure-with-terraform>Accelerating AI adoption on Azure with Terraform (hashicorp.com)</a></p><p><a href=https://github.com/Azure/terraform-azurerm-openai/tree/main>GitHub - Azure/terraform-azurerm-openai: Terraform module for deploying Azure OpenAI Service.</a></p><p><a href=https://opentofu.org/>OpenTofu</a></p><p><a href=https://learn.microsoft.com/en-us/legal/cognitive-services/openai/limited-access>Limited access to Azure OpenAI Service - Azure AI services | Microsoft Learn</a></p><p><a href=https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/guides/azure_cli>Azure Provider: Authenticating via the Azure CLI | Guides | hashicorp/azurerm | Terraform | Terraform Registry</a></p><p><a href=https://github.com/Bin-Huang/chatbox/tree/main>GitHub - Bin-Huang/chatbox: Chatbox is a desktop client for ChatGPT, Claude and other LLMs, available on Windows, Mac, Linux</a></p><p><a href=https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/cognitive_deployment>azurerm_cognitive_deployment | Resources | hashicorp/azurerm | Terraform | Terraform Registry</a></p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://gioleppe.github.io/>Tommaso Colella | SWE</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>