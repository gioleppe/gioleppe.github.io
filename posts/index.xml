<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Tommaso Colella | SWE</title>
    <link>https://gioleppe.github.io/posts/</link>
    <description>Recent content in Posts on Tommaso Colella | SWE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 14 Feb 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://gioleppe.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deploying AzureOpenAI Service using OpenTofu</title>
      <link>https://gioleppe.github.io/posts/azure-openai-opentofu/</link>
      <pubDate>Wed, 14 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/azure-openai-opentofu/</guid>
      <description>Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.
I&amp;rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we&amp;rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I&amp;rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi&amp;rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties.</description>
    </item>
  </channel>
</rss>
