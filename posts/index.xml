<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Tommaso Colella | SWE</title>
    <link>https://gioleppe.github.io/posts/</link>
    <description>Recent content in Posts on Tommaso Colella | SWE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 06 Mar 2024 00:27:25 +0000</lastBuildDate>
    <atom:link href="https://gioleppe.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Enforcing OPA policies with Terraform / OpenTofu in network-isolated Azure VNet - Part 1</title>
      <link>https://gioleppe.github.io/posts/opa-terraform-azure-one/</link>
      <pubDate>Wed, 06 Mar 2024 00:27:25 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/opa-terraform-azure-one/</guid>
      <description>Recently, we had a couple of naming-related incidents at work. We respect certain naming conventions, but sometimes errors slip into our Terraform infrastructure&amp;rsquo;s code, and remediation may be costly.
I had to find a way to simplify resource naming policy enforcement: at first, I thought about using Git Hooks (with a custom HCL parser) or local linters. Both of these approaches came with downsides that would make it unpractical to employ them on multiple repositories: first of all, the necessity to set up hooks or linter configurations in each repo.</description>
    </item>
    <item>
      <title>How to build a Summarizer Bot using GPT4, Azure Functions, GitHub Actions</title>
      <link>https://gioleppe.github.io/posts/summarizer-bot/</link>
      <pubDate>Tue, 27 Feb 2024 16:48:19 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/summarizer-bot/</guid>
      <description>A friend asked me to create a Telegram channel to cross-post my articles onto (I did! You can find it here). The idea seemed nice, but I didn&amp;rsquo;t want to bother with writing a small summary for each article every time, and most of all, forgetful as I am, I would absolutely forget to post to the channel.
I only had one choice: automate everything using GitHub Actions, Azure Function Apps, and Azure OpenAI, trying to have fun and learn something new in the process.</description>
    </item>
    <item>
      <title>Self-hosted Copilot using Google Gemma and Ollama (&#43; Phi-2 comparison)</title>
      <link>https://gioleppe.github.io/posts/gemma-copilot/</link>
      <pubDate>Fri, 23 Feb 2024 21:07:36 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/gemma-copilot/</guid>
      <description>It&amp;rsquo;s impossible to keep up with the rapid developments in the field of LLMs. On Feb 21, Google released a new family of models called Gemma. These models promise top performance for their size. According to the Gemma technical report, the 7B model outperforms all other open-weight models of the same or even bigger sizes, such as Llama-2 13B.
ðŸ’¡According to the technical report accompanying Gemma&amp;rsquo;s announcement, the new models were trained using the same methodologies as Google&amp;rsquo;s top-tier Gemini models</description>
    </item>
    <item>
      <title>DIY self-hosted Copilot using Phi-2 and Ollama</title>
      <link>https://gioleppe.github.io/posts/diy-copilot-phi/</link>
      <pubDate>Mon, 19 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/diy-copilot-phi/</guid>
      <description>On December 12th, Microsoft released their latest &amp;ldquo;SML&amp;rdquo; or &amp;ldquo;Small Language Model&amp;rdquo; Phi-2. This new model is MIT-Licensed. The permissive license makes it a perfect candidate for any experimentation, be it academic or commercial. Phi-2 is also a somewhat &amp;ldquo;green&amp;rdquo; model. The model was trained with a lot less power than some of its bigger cousins LLama-2 or Mistral, to name a few. Well, it&amp;rsquo;s not THAT small (it has 2.</description>
    </item>
    <item>
      <title>Deploying AzureOpenAI Service using OpenTofu</title>
      <link>https://gioleppe.github.io/posts/azure-openai-service-opentofu/</link>
      <pubDate>Wed, 14 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://gioleppe.github.io/posts/azure-openai-service-opentofu/</guid>
      <description>Lately, some friends have asked me to cooperate on a scientific paper regarding LLMs. As part of this collaboration, I had to assess the feasibility of the automatic deployment of an LLM on a public cloud.
I&amp;rsquo;m lucky enough to work at a company focusing on AI and Microsoft-oriented system integration, and we&amp;rsquo;re always looking for new ways to bring value to the market using cutting-edge services. For this reason, I&amp;rsquo;ve been tinkering with the AzureOpenAi Service for a while: it offers business users the possibility to leverage the power of OpenAi&amp;rsquo;s most advanced models, such as GPT-4 Turbo, without sharing confidential organization data with third parties.</description>
    </item>
  </channel>
</rss>
